{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbXLtOymmx1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c36448-c826-4f60-b5d3-8a4dac0f603d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPU detected and configured.\n",
            "âœ… Images loaded per class: {'0_NoCancer': 1180, '1_Cancer': 1178}\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m29084464/29084464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "ğŸ” Training model...\n",
            "Epoch 1/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 2s/step - accuracy: 0.6437 - loss: 1.6549 - val_accuracy: 0.8164 - val_loss: 1.1425\n",
            "Epoch 2/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 243ms/step - accuracy: 0.7958 - loss: 1.1410 - val_accuracy: 0.8305 - val_loss: 0.9453\n",
            "Epoch 3/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.8222 - loss: 0.9310 - val_accuracy: 0.8319 - val_loss: 0.8344\n",
            "Epoch 4/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 212ms/step - accuracy: 0.8222 - loss: 0.8377 - val_accuracy: 0.8305 - val_loss: 0.7824\n",
            "Epoch 5/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 210ms/step - accuracy: 0.8320 - loss: 0.7705 - val_accuracy: 0.8362 - val_loss: 0.7345\n",
            "Epoch 6/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 206ms/step - accuracy: 0.8257 - loss: 0.7410 - val_accuracy: 0.8291 - val_loss: 0.7223\n",
            "Epoch 7/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 215ms/step - accuracy: 0.8507 - loss: 0.6707 - val_accuracy: 0.8333 - val_loss: 0.6852\n",
            "Epoch 8/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 250ms/step - accuracy: 0.8516 - loss: 0.6613 - val_accuracy: 0.8376 - val_loss: 0.6645\n",
            "Epoch 9/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 254ms/step - accuracy: 0.8584 - loss: 0.6324 - val_accuracy: 0.8277 - val_loss: 0.6563\n",
            "Epoch 10/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 258ms/step - accuracy: 0.8664 - loss: 0.6046 - val_accuracy: 0.8376 - val_loss: 0.6393\n",
            "\n",
            "ğŸ¯ Test Accuracy: 83.76%\n",
            "ğŸ“ˆ Final Training Accuracy: 85.27%\n",
            "ğŸ“‰ Final Validation Accuracy: 83.76%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "#detect for gpu\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(\"GPU detected and configured.\")\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n",
        "\n",
        "dataset_dir = '/content/drive/MyDrive/Colab Notebooks/deep learning/breast cancer/cancer dataset'\n",
        "img_size = (224, 224)\n",
        "limit_per_class = 5000\n",
        "\n",
        "#Load Img\n",
        "data = []\n",
        "labels = []\n",
        "class_counts = defaultdict(int)\n",
        "\n",
        "for class_name in sorted(os.listdir(dataset_dir)):\n",
        "    class_path = os.path.join(dataset_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        images = sorted(os.listdir(class_path))[:limit_per_class]\n",
        "        for img_name in images:\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            try:\n",
        "                img = load_img(img_path, target_size=img_size)\n",
        "                img_array = img_to_array(img) / 255.0\n",
        "                data.append(img_array)\n",
        "                labels.append(class_name)\n",
        "                class_counts[class_name] += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {img_path}: {e}\")\n",
        "\n",
        "print(f\"Images loaded per class: {dict(class_counts)}\")\n",
        "\n",
        "#Convert to arrays\n",
        "data = np.array(data, dtype='float32')\n",
        "labels = np.array(labels)\n",
        "\n",
        "#Encode labels\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "num_classes = len(le.classes_)\n",
        "labels = to_categorical(labels_encoded, num_classes=num_classes)\n",
        "\n",
        "#split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "#Load DenseNet121 base\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "#model with regularization\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "    Dropout(0.7),\n",
        "    Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "])\n",
        "\n",
        "#Compile\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "#Train\n",
        "print(\"Training model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Evaluate\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'\\nTest Accuracy: {score[1]*100:.2f}%')\n",
        "print(f'Final Training Accuracy: {history.history[\"accuracy\"][-1]*100:.2f}%')\n",
        "print(f'Final Validation Accuracy: {history.history[\"val_accuracy\"][-1]*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSy7fXEcpTtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}